{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## todo fix classes and labels code in detection.py\n",
    "##tflite tends to give garbage classes to null detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE - TEST MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float32'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bdd046e9cbd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m \u001b[0mstreaming\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-bdd046e9cbd9>\u001b[0m in \u001b[0;36mstreaming\u001b[1;34m()\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m                     \u001b[0mresults\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_inference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m                     \u001b[0mresults\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_boxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdet_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\adappt-overhead\\footfall_counter\\people_counting\\tf_inference\\detection.py\u001b[0m in \u001b[0;36mmodel_inference\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmodel_inference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtflite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtflite_inference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_inference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\adappt-overhead\\footfall_counter\\people_counting\\tf_inference\\detection.py\u001b[0m in \u001b[0;36mtflite_inference\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mboxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tflite_runtime\\interpreter.py\u001b[0m in \u001b[0;36minvoke\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    505\u001b[0m     \"\"\"\n\u001b[0;32m    506\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_safe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 507\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interpreter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvoke\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_all_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tflite_runtime\\tensorflow_wrap_interpreter_wrapper.py\u001b[0m in \u001b[0;36mInvoke\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mInvoke\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_tensorflow_wrap_interpreter_wrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInterpreterWrapper_Invoke\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mInputIndices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from streamutils import *\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def adjust_gamma(image, gamma=1.0):\n",
    "\t# build a lookup table mapping the pixel values [0, 255] to\n",
    "\t# their adjusted gamma values\n",
    "\tinvGamma = 1.0 / gamma\n",
    "\ttable = np.array([((i / 255.0) ** invGamma) * 255\n",
    "\t\tfor i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "\t# apply gamma correction using the lookup table\n",
    "\treturn cv2.LUT(image, table)\n",
    "\n",
    "def anonymize_face_pixelate(image, blocks=3):\n",
    "\t# divide the input image into NxN blocks\n",
    "\t(h, w) = image.shape[:2]\n",
    "\txSteps = np.linspace(0, w, blocks + 1, dtype=\"int\")\n",
    "\tySteps = np.linspace(0, h, blocks + 1, dtype=\"int\")\n",
    "\t# loop over the blocks in both the x and y direction\n",
    "\tfor i in range(1, len(ySteps)):\n",
    "\t\tfor j in range(1, len(xSteps)):\n",
    "\t\t\t# compute the starting and ending (x, y)-coordinates\n",
    "\t\t\t# for the current block\n",
    "\t\t\tstartX = xSteps[j - 1]\n",
    "\t\t\tstartY = ySteps[i - 1]\n",
    "\t\t\tendX = xSteps[j]\n",
    "\t\t\tendY = ySteps[i]\n",
    "\t\t\t# extract the ROI using NumPy array slicing, compute the\n",
    "\t\t\t# mean of the ROI, and then draw a rectangle with the\n",
    "\t\t\t# mean RGB values over the ROI in the original image\n",
    "\t\t\troi = image[startY:endY, startX:endX]\n",
    "\t\t\t(B, G, R) = [int(x) for x in cv2.mean(roi)[:3]]\n",
    "\t\t\tcv2.rectangle(image, (startX, startY), (endX, endY),\n",
    "\t\t\t\t(B, G, R), -1)\n",
    "\t# return the pixelated blurred image\n",
    "\treturn image\n",
    "\n",
    "\n",
    "def streaming():\n",
    "    \n",
    "    config=readConfig()\n",
    "    DB().createDB()\n",
    "    \n",
    "    model_kwargs=dict(config['model'])\n",
    "    \n",
    "    record=config['video']['record']\n",
    "    draw=config['video']['draw']\n",
    "    display=config['video']['display']\n",
    "    test=config['video']['test']\n",
    "    color=config['image']['color']\n",
    "    \n",
    "    \n",
    "    #test_video=config['video']['test_video'] if test is not None else None\n",
    "    test_video=config['video']['test_video'] if test else None\n",
    "    \n",
    "    schedule.every().second.do(DB().limitDB,config['db']['max_days']) \n",
    "    \n",
    "    track,detection=True,True\n",
    "    if not draw:\n",
    "        track,detection=False,False\n",
    "    \n",
    "    #initialize model detection\n",
    "    if detection:\n",
    "        det=Detection()\n",
    "        det.load_model(**model_kwargs)\n",
    "        resolution=det.resolution\n",
    "        \n",
    "        if track:\n",
    "            motrk=MotionTracker(config['motion']['frequency'])\n",
    "            trk=Tracking(resolution)\n",
    "     \n",
    "    if record:\n",
    "        vid,output_path=recorder(test_video,model_kwargs['model_path'],resolution)\n",
    "        \n",
    "    \n",
    "    for frame,fps in stream_video(test_video,resolution):\n",
    "        config=readConfig()\n",
    "        det_kwargs=dict(config['detection'])\n",
    "        counter_kwargs=dict(config['counter'])\n",
    "        track_kwargs=dict(config['tracking'])\n",
    "        \n",
    "        \n",
    "        frame,image=det.transform_image(frame,resolution,color)\n",
    "        \n",
    "        \n",
    "        if detection:\n",
    "            if not motrk.check_count():\n",
    "\n",
    "\n",
    "                    results=det.model_inference(image)\n",
    "\n",
    "                    results=det.filter_boxes(results,**det_kwargs)\n",
    "\n",
    "                    results=det.scale_boxes(results,resolution)\n",
    "                    \n",
    "                    if config['motion']['frequency']>0:\n",
    "                        motrk.start_trackers(image,results[0])\n",
    "\n",
    "            else:\n",
    "\n",
    "                results=motrk.update_trackers(image)\n",
    "            \n",
    "        \n",
    "        if detection and track:\n",
    "            \n",
    "            trk.track_objects(results[0],counter_kwargs['line_points'],**track_kwargs)\n",
    "           \n",
    "            \n",
    "            (on_update,output_data)=trk.update_counter(config['location']['capacity'],counter_kwargs['entrance'],\n",
    "                                                       counter_kwargs['minutes_inactive'],  \n",
    "                               counter_kwargs['percent_cap'],counter_kwargs['min_wait_time'],counter_kwargs['max_wait_time'],\n",
    "                               counter_kwargs['reset'])\n",
    "            \n",
    "            if on_update:\n",
    "                \n",
    "                DB().writeDbData(output_data)\n",
    "\n",
    "            \n",
    "        #if fps is not None:\n",
    "        #    fps.stop()\n",
    "        #    curr_fps=round(fps.fps(),2)\n",
    "        apply_filter=True\n",
    "        if apply_filter:\n",
    "            frame=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "            frame=1-frame\n",
    "            frame=adjust_gamma(frame,gamma=0.6)\n",
    "            #frame=anonymize_face_pixelate(frame,110)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)\n",
    "            \n",
    "        \n",
    "\n",
    "        \n",
    "        if record or display:\n",
    "            frame=det.draw_boxes(frame,results,det.labels)    \n",
    "            \n",
    "            if track:\n",
    "                frame=trk.draw_tracking(frame)\n",
    "\n",
    "            if record:\n",
    "                vid.write(frame)\n",
    "                \n",
    "            if display:\n",
    "                cv2.imshow('imgs',frame)\n",
    "            \n",
    "        if cv2.waitKey(1) == ord('q'):break \n",
    "            \n",
    "        schedule.run_pending() \n",
    "                \n",
    "    if record:\n",
    "        vid.release()\n",
    "        \n",
    "        output_split=os.path.splitext(output_path)\n",
    "        new_output_path='{}_{:.1f}{}'.format(output_split[0],fps.fps(),output_split[-1])\n",
    "        os.rename(output_path,new_output_path)\n",
    "     \n",
    "        \n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "streaming()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "def anonymize_face_pixelate(image, blocks=3):\n",
    "\t# divide the input image into NxN blocks\n",
    "\t(h, w) = image.shape[:2]\n",
    "\txSteps = np.linspace(0, w, blocks + 1, dtype=\"int\")\n",
    "\tySteps = np.linspace(0, h, blocks + 1, dtype=\"int\")\n",
    "\t# loop over the blocks in both the x and y direction\n",
    "\tfor i in range(1, len(ySteps)):\n",
    "\t\tfor j in range(1, len(xSteps)):\n",
    "\t\t\t# compute the starting and ending (x, y)-coordinates\n",
    "\t\t\t# for the current block\n",
    "\t\t\tstartX = xSteps[j - 1]\n",
    "\t\t\tstartY = ySteps[i - 1]\n",
    "\t\t\tendX = xSteps[j]\n",
    "\t\t\tendY = ySteps[i]\n",
    "\t\t\t# extract the ROI using NumPy array slicing, compute the\n",
    "\t\t\t# mean of the ROI, and then draw a rectangle with the\n",
    "\t\t\t# mean RGB values over the ROI in the original image\n",
    "\t\t\troi = image[startY:endY, startX:endX]\n",
    "\t\t\t(B, G, R) = [int(x) for x in cv2.mean(roi)[:3]]\n",
    "\t\t\tcv2.rectangle(image, (startX, startY), (endX, endY),\n",
    "\t\t\t\t(B, G, R), -1)\n",
    "\t# return the pixelated blurred image\n",
    "\treturn image\n",
    "\n",
    "def adjust_gamma(image, gamma=1.0):\n",
    "\t# build a lookup table mapping the pixel values [0, 255] to\n",
    "\t# their adjusted gamma values\n",
    "\tinvGamma = 1.0 / gamma\n",
    "\ttable = np.array([((i / 255.0) ** invGamma) * 255\n",
    "\t\tfor i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "\t# apply gamma correction using the lookup table\n",
    "\treturn cv2.LUT(image, table)\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture('videos/video-4.mp4')\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3))\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "while(1):\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    \n",
    "    \n",
    "    edges = cv2.Canny(frame,50,200)\n",
    "    gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #sobelx = cv2.Laplacian(gray,cv2.CV_64F)\n",
    "    #sobelx =cv2.Sobel(gray,cv2.CV_64F,1,0,ksize=1)\n",
    "    \n",
    "    \n",
    "    anonymous=anonymize_face_pixelate(gray.copy(),50)\n",
    "    \n",
    "    \n",
    "    fgmask = fgbg.apply(frame)\n",
    "    fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    #fgmask=gray+edges\n",
    "    \n",
    "    \n",
    "    \n",
    "    fgmask=1-gray\n",
    "    fgmask=adjust_gamma(fgmask,gamma=0.5)\n",
    "    fgmask=anonymize_face_pixelate(fgmask,110)\n",
    "    #fgmask+=anonymous\n",
    "    \n",
    "    cv2.imshow('frame',fgmask)\n",
    "    \n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_gamma(image, gamma=1.0):\n",
    "\t# build a lookup table mapping the pixel values [0, 255] to\n",
    "\t# their adjusted gamma values\n",
    "\tinvGamma = 1.0 / gamma\n",
    "\ttable = np.array([((i / 255.0) ** invGamma) * 255\n",
    "\t\tfor i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "\t# apply gamma correction using the lookup table\n",
    "\treturn cv2.LUT(image, table)\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "cap = cv2.VideoCapture(\"videos/video-4.mp4\")\n",
    "\n",
    "ret, frame1 = cap.read()\n",
    "prvs = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)\n",
    "hsv = np.zeros_like(frame1)\n",
    "hsv[...,1] = 255\n",
    "\n",
    "while(1):\n",
    "    ret, frame2 = cap.read()\n",
    "    \n",
    "    \n",
    "    #fgmask=anonymize_face_pixelate(fgmask,110)\n",
    "    \n",
    "    next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)\n",
    "    next=1-next\n",
    "    next=adjust_gamma(next,gamma=0.5)\n",
    "\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "    mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])\n",
    "    hsv[...,0] = ang*180/np.pi/2\n",
    "    hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n",
    "    rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)\n",
    "    \n",
    "    rgb=cv2.cvtColor(next,cv2.COLOR_GRAY2BGR)#+rgb\n",
    "\n",
    "    cv2.imshow('frame2',rgb)\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "    elif k == ord('s'):\n",
    "        cv2.imwrite('opticalfb.png',frame2)\n",
    "        cv2.imwrite('opticalhsv.png',rgb)\n",
    "    prvs = next\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## For converting mobnet3 to a valid inference graph\n",
    "2**16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.4], [1.0, 0.4]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resolution=(640,480)\n",
    "angle=0\n",
    "\n",
    "line_points=[[0, int(2*(resolution[1]/5))-angle],[resolution[0], int(2*(resolution[1]/5))+angle]]\n",
    "line_points[0][0]=line_points[0][0]/resolution[0]\n",
    "line_points[0][1]=line_points[0][1]/resolution[1]\n",
    "line_points[1][0]=line_points[1][0]/resolution[0]\n",
    "line_points[1][1]=line_points[1][1]/resolution[1]\n",
    "line_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "68.667==18 in\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8148333333333335"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "68.667/18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68.58"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3.81*18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2621346498317969"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "18/68.667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "ls=[0,1]\n",
    "for l in zip(ls,ls):\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
